{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度总体分析: 汇总版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import mask\n",
    "from brainspace.gradient import GradientMaps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_rel\n",
    "from scipy.stats import ttest_ind\n",
    "from mytoolkit2 import gradient_toolkit\n",
    "from mytoolkit1.ttest import ind_cohen_d, rel_cohen_d\n",
    "\n",
    "n_conponents = 20\n",
    "approach = 'pca'\n",
    "kernel = 'cosine'\n",
    "alignment='procrustes'\n",
    "sparsity = 0.80\n",
    "save_csv = True\n",
    "save_nii = True\n",
    "\n",
    "ourdir = r\"E:\\006_ET_MRgFUS\\13_gradient_batch2.2\\04_pca_0.80\" ##########################################################################################################################\n",
    "m = mask.NiiMask(r\"E:\\006_ET_MRgFUS\\09_gradient_batch2\\02_nii_mask\\1039atlas.nii\")\n",
    "\n",
    "\n",
    "# generate template gradient for alignment\n",
    "gm_temp = gradient_toolkit.generate_template_gradientmap(r'E:\\006_ET_MRgFUS\\09_gradient_batch2\\01_zform_data_group\\unused_hc',\n",
    "                                      n_conponents, kernel, sparsity)\n",
    "# generate aligned gradient maps for different groups by applying template gradient\n",
    "\n",
    "gm_HCHCHC, files_HCHCHC = gradient_toolkit.get_aligned_gradientmaps(r'E:\\006_ET_MRgFUS\\09_gradient_batch2\\01_zform_data_group\\ncnc',\n",
    "                                  n_conponents, approach, kernel, alignment, sparsity, gm_temp)\n",
    "gm_ET000d, files_ET000d = gradient_toolkit.get_aligned_gradientmaps(r'E:\\006_ET_MRgFUS\\09_gradient_batch2\\01_zform_data_group\\000d',\n",
    "                                  n_conponents, approach, kernel, alignment, sparsity, gm_temp)\n",
    "gm_ET180d, files_ET180d = gradient_toolkit.get_aligned_gradientmaps(r'E:\\006_ET_MRgFUS\\09_gradient_batch2\\01_zform_data_group\\180d',\n",
    "                                  n_conponents, approach, kernel, alignment, sparsity, gm_temp)\n",
    "\n",
    "txtpath = os.path.join(ourdir,\"general_result.txt\") ##########################################################################################################################\n",
    "\n",
    "with open(txtpath, 'w') as fff:\n",
    "\n",
    "    for i in range(2):\n",
    "\n",
    "        print(\"i=\"+str(i))\n",
    "\n",
    "        lam_HCHCHC = np.array(gm_HCHCHC.lambdas_)\n",
    "        lam_ET180d = np.array(gm_ET180d.lambdas_)\n",
    "        lam_ET000d = np.array(gm_ET000d.lambdas_)\n",
    "        \n",
    "\n",
    "        var_ET180d = lam_ET180d[:, i]/np.sum(lam_ET180d, axis=1)\n",
    "        var_ET000d = lam_ET000d[:, i]/np.sum(lam_ET000d, axis=1)\n",
    "        var_HCHCHC = lam_HCHCHC[:, i]/np.sum(lam_HCHCHC, axis=1)\n",
    "\n",
    "        ts, ps = ttest_rel(var_ET180d, var_ET000d) \n",
    "        d = rel_cohen_d(var_ET180d, var_ET000d) \n",
    "        print(f'ET180d_vs_ET000d var_ET180d:{np.mean(var_ET180d):.2f}, var_ET000d:{np.mean(var_ET000d):.2f}, var_t&p:{ts},{ps}, var_d:{d}', file=fff)\n",
    "\n",
    "        ts, ps = ttest_ind(var_ET000d, var_HCHCHC)\n",
    "        d = ind_cohen_d(var_ET000d, var_HCHCHC)\n",
    "        print(f'ET000d_vs_HCHCHC var_ET000d:{np.mean(var_ET000d):.2f}, var_HCHCHC:{np.mean(var_HCHCHC):.2f}, var_t&p:{ts},{ps}, var_d:{d}', file=fff)\n",
    "\n",
    "        \n",
    "        ts, ps = ttest_ind(var_ET180d, var_HCHCHC)\n",
    "        d = ind_cohen_d(var_ET180d, var_HCHCHC)\n",
    "        print(f'ET180d_vs_HCHCHC var_ET180d:{np.mean(var_ET180d):.2f}, var_HCHCHC:{np.mean(var_HCHCHC):.2f}, var_t&p:{ts},{ps}, var_d:{d}', file=fff)\n",
    "\n",
    "nth_gradient = 0 \n",
    "\n",
    "_01_dir = os.path.join(ourdir,\"01_primary_gradient_aligned\")#####################################################################################################\n",
    "if not os.path.exists(_01_dir):\n",
    "    os.mkdir(_01_dir)\n",
    "_02_dir = os.path.join(ourdir,\"02_primary_gradient_unaligned\")#####################################################################################################\n",
    "if not os.path.exists(_02_dir):\n",
    "    os.mkdir(_02_dir)\n",
    "\n",
    "\n",
    "gradient_toolkit.save_gradients(_01_dir, files_ET000d, gm_ET000d.aligned_, nth_gradient=nth_gradient, save_csv=save_csv, save_nii=save_nii, mask=m)\n",
    "gradient_toolkit.save_gradients(_01_dir, files_ET180d, gm_ET180d.aligned_, nth_gradient=nth_gradient, save_csv=save_csv, save_nii=save_nii, mask=m)\n",
    "gradient_toolkit.save_gradients(_01_dir, files_HCHCHC, gm_HCHCHC.aligned_, nth_gradient=nth_gradient, save_csv=save_csv, save_nii=save_nii, mask=m)\n",
    "\n",
    "gradient_toolkit.save_gradients(_02_dir, files_ET000d, gm_ET000d.gradients_, nth_gradient=nth_gradient, save_csv=save_csv, save_nii=save_nii, mask=m)\n",
    "gradient_toolkit.save_gradients(_02_dir, files_ET180d, gm_ET180d.gradients_, nth_gradient=nth_gradient, save_csv=save_csv, save_nii=save_nii, mask=m)\n",
    "gradient_toolkit.save_gradients(_02_dir, files_HCHCHC, gm_HCHCHC.gradients_, nth_gradient=nth_gradient, save_csv=save_csv, save_nii=save_nii, mask=m)\n",
    "\n",
    "nth_gradient = 1 \n",
    "\n",
    "_05_dir = os.path.join(ourdir,\"05_second_gradient_aligned\")#####################################################################################################\n",
    "if not os.path.exists(_05_dir):\n",
    "    os.mkdir(_05_dir)\n",
    "_06_dir = os.path.join(ourdir,\"06_second_gradient_unaligned\")#####################################################################################################\n",
    "if not os.path.exists(_06_dir):\n",
    "    os.mkdir(_06_dir)\n",
    "\n",
    "\n",
    "gradient_toolkit.save_gradients(_05_dir, files_ET000d, gm_ET000d.aligned_, nth_gradient=nth_gradient, save_csv=save_csv, save_nii=save_nii, mask=m)\n",
    "gradient_toolkit.save_gradients(_05_dir, files_ET180d, gm_ET180d.aligned_, nth_gradient=nth_gradient, save_csv=save_csv, save_nii=save_nii, mask=m)\n",
    "gradient_toolkit.save_gradients(_05_dir, files_HCHCHC, gm_HCHCHC.aligned_, nth_gradient=nth_gradient, save_csv=save_csv, save_nii=save_nii, mask=m)\n",
    "\n",
    "gradient_toolkit.save_gradients(_06_dir, files_ET000d, gm_ET000d.gradients_, nth_gradient=nth_gradient, save_csv=save_csv, save_nii=save_nii, mask=m)\n",
    "gradient_toolkit.save_gradients(_06_dir, files_ET180d, gm_ET180d.gradients_, nth_gradient=nth_gradient, save_csv=save_csv, save_nii=save_nii, mask=m)\n",
    "gradient_toolkit.save_gradients(_06_dir, files_HCHCHC, gm_HCHCHC.gradients_, nth_gradient=nth_gradient, save_csv=save_csv, save_nii=save_nii, mask=m)\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from datasets import mask\n",
    "from mytoolkit2 import gradient_toolkit\n",
    "\n",
    "def save_mean_gradient(in_dir, out_dir, fliter, m):\n",
    "    files = os.listdir(in_dir)\n",
    "    files = [f for f in files if f'{fliter}.csv' in f] \n",
    "    dfs = gradient_toolkit.load_gradient_csvs(in_dir, files)\n",
    "    gradients = [df['gradient'].values for df in dfs]\n",
    "    mean_gradients = np.mean(gradients, axis=0)\n",
    "    gradient_toolkit.save_gradient(out_dir, f'Mean_{fliter}_aligned', mean_gradients, save_csv=True, save_nii=True, mask=m) \n",
    "\n",
    "_03_dir = os.path.join(ourdir,\"03_primary_gradient_aligned_mean_ROI\")#####################################################################################################\n",
    "if not os.path.exists(_03_dir):\n",
    "    os.mkdir(_03_dir)\n",
    "_07_dir = os.path.join(ourdir,\"07_second_gradient_aligned_mean_ROI\")#####################################################################################################\n",
    "if not os.path.exists(_07_dir):\n",
    "    os.mkdir(_07_dir)\n",
    "\n",
    "save_mean_gradient(_01_dir, _03_dir, 'ncnc', m)\n",
    "save_mean_gradient(_01_dir, _03_dir, \"180d\", m)\n",
    "save_mean_gradient(_01_dir, _03_dir, \"000d\", m)\n",
    "save_mean_gradient(_05_dir, _07_dir, 'ncnc', m)\n",
    "save_mean_gradient(_05_dir, _07_dir, \"180d\", m)\n",
    "save_mean_gradient(_05_dir, _07_dir, \"000d\", m)\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import ttest_rel\n",
    "from datasets import mask\n",
    "from mytoolkit2 import gradient_toolkit\n",
    "\n",
    "_Gradient_1_dir = os.path.join(ourdir,\"Gradient-1\")\n",
    "if not os.path.exists(_Gradient_1_dir):\n",
    "    os.mkdir(_Gradient_1_dir)\n",
    "\n",
    "txtpath = os.path.join(_Gradient_1_dir,\"Gradient-1-comparation.txt\")\n",
    "\n",
    "with open(txtpath, 'w') as ggg:\n",
    "    files = os.listdir(_01_dir)\n",
    "    files1 = [f for f in files if '180d.csv' in f]\n",
    "    files2 = [f for f in files if '000d.csv' in f]\n",
    "\n",
    "    # Calculate ttest\n",
    "    dfs1 = gradient_toolkit.load_gradient_csvs(_01_dir, files1)\n",
    "    gradients1 = [df['gradient'].values for df in dfs1]\n",
    "\n",
    "    dfs2 = gradient_toolkit.load_gradient_csvs(_01_dir, files2)\n",
    "    gradients2 = [df['gradient'].values for df in dfs2]\n",
    "\n",
    "    ts, ps = ttest_rel(gradients1, gradients2)\n",
    "\n",
    "    ts_filtered = np.multiply(ts, ps<0.05)\n",
    "    tp = dict(zip(range(1, np.shape(ts)[0]+1), ts_filtered))\n",
    "\n",
    "    print(f'ET180d_vs_ET000d Significant ROI count:{np.sum(ps<0.05)}', file=ggg) \n",
    "\n",
    "    m_path = os.path.join(_Gradient_1_dir,\"Part3.2.1_ET180d_vs_ET000d_tvalue.nii\")\n",
    "    m.save_values(tp, out_path=m_path) \n",
    "\n",
    "    df = pd.DataFrame({'roi': range(1, np.shape(ts)[0]+1), 't_value': ts, 'p_value': ps})\n",
    "    df.to_csv(os.path.join(_Gradient_1_dir,'Part3.2.2_ET180d_vs_ET000d.csv')) \n",
    "\n",
    "    ####################################################################################################\n",
    "\n",
    "    files = os.listdir(_01_dir)\n",
    "    files1 = [f for f in files if '000d.csv' in f]\n",
    "    files2 = [f for f in files if 'ncnc.csv' in f]\n",
    "\n",
    "    # Calculate ttest\n",
    "    dfs1 = gradient_toolkit.load_gradient_csvs(_01_dir, files1)\n",
    "    gradients1 = [df['gradient'].values for df in dfs1]\n",
    "\n",
    "    dfs2 = gradient_toolkit.load_gradient_csvs(_01_dir, files2)\n",
    "    gradients2 = [df['gradient'].values for df in dfs2]\n",
    "\n",
    "    ts, ps = ttest_ind(gradients1, gradients2) #ttest_rel\n",
    "\n",
    "    ts_filtered = np.multiply(ts, ps<0.05)\n",
    "    tp = dict(zip(range(1, np.shape(ts)[0]+1), ts_filtered))\n",
    "\n",
    "    print(f'ET000d_vs_HCHCHC Significant ROI count:{np.sum(ps<0.05)}', file=ggg) \n",
    "\n",
    "    m_path = os.path.join(_Gradient_1_dir,\"Part3.2.3_ET000d_vs_HCHCHC_tvalue.nii\")\n",
    "    m.save_values(tp, out_path=m_path) \n",
    "    \n",
    "    df = pd.DataFrame({'roi': range(1, np.shape(ts)[0]+1), 't_value': ts, 'p_value': ps})\n",
    "    df.to_csv(os.path.join(_Gradient_1_dir,'Part3.2.4_ET000d_vs_HCHCHC.csv')) \n",
    "####################################################################################################\n",
    "\n",
    "    files = os.listdir(_01_dir)\n",
    "    files1 = [f for f in files if '180d.csv' in f]\n",
    "    files2 = [f for f in files if 'ncnc.csv' in f]\n",
    "\n",
    "    # Calculate ttest\n",
    "    dfs1 = gradient_toolkit.load_gradient_csvs(_01_dir, files1)\n",
    "    gradients1 = [df['gradient'].values for df in dfs1]\n",
    "\n",
    "    dfs2 = gradient_toolkit.load_gradient_csvs(_01_dir, files2)\n",
    "    gradients2 = [df['gradient'].values for df in dfs2]\n",
    "\n",
    "    ts, ps = ttest_ind(gradients1, gradients2)\n",
    "\n",
    "    ts_filtered = np.multiply(ts, ps<0.05)\n",
    "    tp = dict(zip(range(1, np.shape(ts)[0]+1), ts_filtered))\n",
    "\n",
    "    print(f'ET180d_vs_HCHCHC Significant ROI count:{np.sum(ps<0.05)}', file=ggg) \n",
    "\n",
    "    m_path = os.path.join(_Gradient_1_dir,\"Part3.2.5_ET180d_vs_HCHCHC_tvalue.nii\")\n",
    "    m.save_values(tp, out_path=m_path) \n",
    "\n",
    "    df = pd.DataFrame({'roi': range(1, np.shape(ts)[0]+1), 't_value': ts, 'p_value': ps})\n",
    "    df.to_csv(os.path.join(_Gradient_1_dir,'Part3.2.6_ET180d_vs_HCHCHC.csv')) \n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import ttest_rel\n",
    "from datasets import mask\n",
    "from mytoolkit2 import gradient_toolkit\n",
    "\n",
    "_Gradient_2_dir = os.path.join(ourdir,\"Gradient-2\")\n",
    "if not os.path.exists(_Gradient_2_dir):\n",
    "    os.mkdir(_Gradient_2_dir)\n",
    "\n",
    "txtpath = os.path.join(_Gradient_2_dir,\"Gradient-2-comparation.txt\")\n",
    "\n",
    "with open(txtpath, 'w') as ggg:\n",
    "    files = os.listdir(_05_dir)\n",
    "    files1 = [f for f in files if '180d.csv' in f]\n",
    "    files2 = [f for f in files if '000d.csv' in f]\n",
    "\n",
    "    # Calculate ttest\n",
    "    dfs1 = gradient_toolkit.load_gradient_csvs(_05_dir, files1)\n",
    "    gradients1 = [df['gradient'].values for df in dfs1]\n",
    "\n",
    "    dfs2 = gradient_toolkit.load_gradient_csvs(_05_dir, files2)\n",
    "    gradients2 = [df['gradient'].values for df in dfs2]\n",
    "\n",
    "    ts, ps = ttest_rel(gradients1, gradients2)\n",
    "\n",
    "    ts_filtered = np.multiply(ts, ps<0.05)\n",
    "    tp = dict(zip(range(1, np.shape(ts)[0]+1), ts_filtered))\n",
    "\n",
    "    print(f'ET180d_vs_ET000d Significant ROI count:{np.sum(ps<0.05)}', file=ggg) \n",
    "\n",
    "    m_path = os.path.join(_Gradient_2_dir,\"Part3.2.1_ET180d_vs_ET000d_tvalue.nii\")\n",
    "    m.save_values(tp, out_path=m_path) \n",
    "\n",
    "    df = pd.DataFrame({'roi': range(1, np.shape(ts)[0]+1), 't_value': ts, 'p_value': ps})\n",
    "    df.to_csv(os.path.join(_Gradient_2_dir,'Part3.2.2_ET180d_vs_ET000d.csv')) \n",
    "\n",
    "    ####################################################################################################\n",
    "\n",
    "    files = os.listdir(_05_dir)\n",
    "    files1 = [f for f in files if '000d.csv' in f]\n",
    "    files2 = [f for f in files if 'ncnc.csv' in f]\n",
    "\n",
    "    # Calculate ttest\n",
    "    dfs1 = gradient_toolkit.load_gradient_csvs(_05_dir, files1)\n",
    "    gradients1 = [df['gradient'].values for df in dfs1]\n",
    "\n",
    "    dfs2 = gradient_toolkit.load_gradient_csvs(_05_dir, files2)\n",
    "    gradients2 = [df['gradient'].values for df in dfs2]\n",
    "\n",
    "    ts, ps = ttest_ind(gradients1, gradients2) #ttest_rel\n",
    "\n",
    "    ts_filtered = np.multiply(ts, ps<0.05)\n",
    "    tp = dict(zip(range(1, np.shape(ts)[0]+1), ts_filtered))\n",
    "\n",
    "    print(f'ET000d_vs_HCHCHC Significant ROI count:{np.sum(ps<0.05)}', file=ggg) \n",
    "\n",
    "    m_path = os.path.join(_Gradient_2_dir,\"Part3.2.3_ET000d_vs_HCHCHC_tvalue.nii\")\n",
    "    m.save_values(tp, out_path=m_path) \n",
    "  \n",
    "    df = pd.DataFrame({'roi': range(1, np.shape(ts)[0]+1), 't_value': ts, 'p_value': ps})\n",
    "    df.to_csv(os.path.join(_Gradient_2_dir,'Part3.2.4_ET000d_vs_HCHCHC.csv')) \n",
    "####################################################################################################\n",
    "\n",
    "    files = os.listdir(_05_dir)\n",
    "    files1 = [f for f in files if '180d.csv' in f]\n",
    "    files2 = [f for f in files if 'ncnc.csv' in f]\n",
    "\n",
    "    # Calculate ttest\n",
    "    dfs1 = gradient_toolkit.load_gradient_csvs(_05_dir, files1)\n",
    "    gradients1 = [df['gradient'].values for df in dfs1]\n",
    "\n",
    "    dfs2 = gradient_toolkit.load_gradient_csvs(_05_dir, files2)\n",
    "    gradients2 = [df['gradient'].values for df in dfs2]\n",
    "\n",
    "    ts, ps = ttest_ind(gradients1, gradients2)\n",
    "\n",
    "    ts_filtered = np.multiply(ts, ps<0.05)\n",
    "    tp = dict(zip(range(1, np.shape(ts)[0]+1), ts_filtered))\n",
    "\n",
    "    print(f'ET180d_vs_HCHCHC Significant ROI count:{np.sum(ps<0.05)}', file=ggg) \n",
    "\n",
    "    m_path = os.path.join(_Gradient_2_dir,\"Part3.2.5_ET180d_vs_HCHCHC_tvalue.nii\")\n",
    "    m.save_values(tp, out_path=m_path) \n",
    "\n",
    "    df = pd.DataFrame({'roi': range(1, np.shape(ts)[0]+1), 't_value': ts, 'p_value': ps})\n",
    "    df.to_csv(os.path.join(_Gradient_2_dir,'Part3.2.6_ET180d_vs_HCHCHC.csv')) \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "subnetwork_path = r'E:\\006_ET_MRgFUS\\09_gradient_batch2\\02_nii_mask\\7network_atlas1039.csv'\n",
    "subnetwork_df = pd.read_csv(subnetwork_path)\n",
    "subnetwork_label = subnetwork_df['Subnetwork'].values\n",
    "\n",
    "# x ->gradient2, y->gradient1\n",
    "\n",
    "path1 = os.path.join(_03_dir,\"Mean_ncnc_aligned.csv\")\n",
    "path2 = os.path.join(_07_dir,\"Mean_ncnc_aligned.csv\")\n",
    "savepath = os.path.join(ourdir,\"Mean_ncnc_aligned.png\")\n",
    "\n",
    "aaa = -5\n",
    "bbb = 7\n",
    "ccc = -4\n",
    "ddd = 6.5\n",
    "\n",
    "df1 = pd.read_csv(path1, index_col=0)\n",
    "df2 = pd.read_csv(path2, index_col=0)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim(aaa,bbb)\n",
    "ax.set_ylim(ccc,ddd)\n",
    "\n",
    "x = df1['gradient'].values\n",
    "y = df2['gradient'].values\n",
    "\n",
    "print(x.shape, y.shape, subnetwork_label.shape)\n",
    "\n",
    "ax.set_xlabel(\"Gradient1\")\n",
    "ax.set_ylabel(\"Gradient2\")\n",
    "\n",
    "sns.scatterplot(x=x,  y=y, hue=subnetwork_label, palette=\"deep\",) \n",
    "plt.savefig(savepath)\n",
    "#plt.show()\n",
    "plt.close()\n",
    "\n",
    "############################################\n",
    "\n",
    "path1 = os.path.join(_03_dir,\"Mean_000d_aligned.csv\")\n",
    "path2 = os.path.join(_07_dir,\"Mean_000d_aligned.csv\")\n",
    "savepath = os.path.join(ourdir,\"Mean_000d_aligned.png\")\n",
    "\n",
    "df1 = pd.read_csv(path1, index_col=0)\n",
    "df2 = pd.read_csv(path2, index_col=0)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim(aaa,bbb)\n",
    "ax.set_ylim(ccc,ddd)\n",
    "\n",
    "x = df1['gradient'].values\n",
    "y = df2['gradient'].values\n",
    "\n",
    "ax.set_xlabel(\"Gradient1\")\n",
    "ax.set_ylabel(\"Gradient2\")\n",
    "\n",
    "sns.scatterplot(x=x,  y=y, hue=subnetwork_label, palette=\"deep\",) \n",
    "plt.savefig(savepath)\n",
    "#plt.show()\n",
    "plt.close()\n",
    "\n",
    "############################################\n",
    "\n",
    "path1 = os.path.join(_03_dir,\"Mean_180d_aligned.csv\")\n",
    "path2 = os.path.join(_07_dir,\"Mean_180d_aligned.csv\")\n",
    "savepath = os.path.join(ourdir,\"Mean_180d_aligned.png\")\n",
    "\n",
    "df1 = pd.read_csv(path1, index_col=0)\n",
    "df2 = pd.read_csv(path2, index_col=0)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim(aaa,bbb)\n",
    "ax.set_ylim(ccc,ddd)\n",
    "\n",
    "x = df1['gradient'].values\n",
    "y = df2['gradient'].values\n",
    "\n",
    "ax.set_xlabel(\"Gradient1\")\n",
    "ax.set_ylabel(\"Gradient2\")\n",
    "\n",
    "sns.scatterplot(x=x,  y=y, hue=subnetwork_label, palette=\"deep\",) \n",
    "plt.savefig(savepath)\n",
    "#plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6eac6ea3f87aa8a18101560350ba9d76665d8c267d107134f193807daf6a975"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
