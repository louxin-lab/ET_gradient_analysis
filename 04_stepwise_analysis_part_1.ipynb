{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mytoolkit2 import stepwise\n",
    "import numpy as np\n",
    "\n",
    "def cal_wqs(input_dir, thres, step):\n",
    "    files = os.listdir(input_dir)\n",
    "    wqs = []\n",
    "\n",
    "    for f in files:\n",
    "        array= np.loadtxt(os.path.join(input_dir, f))\n",
    "        #array[array<0] = 0\n",
    "       \n",
    "        #array = stepwise.sparsity(array, thres=thres)\n",
    "        array = stepwise.row_sparsity(array, thres=thres)\n",
    "      \n",
    "        wq, *_ = stepwise.findwalks_v2(array, step, normalize=stepwise.min_max)\n",
    "        wqs.append(wq)\n",
    "    return wqs\n",
    "\n",
    "input_dir1 = r'E:\\006_ET_MRgFUS\\09_gradient_batch2\\01_zform_data_group\\ncnc'\n",
    "input_dir2 = r'E:\\006_ET_MRgFUS\\09_gradient_batch2\\01_zform_data_group\\000d'\n",
    "input_dir3 = r'E:\\006_ET_MRgFUS\\09_gradient_batch2\\01_zform_data_group\\180d'\n",
    "\n",
    "wqs_ncnc = cal_wqs(input_dir1, 0.90, 100)\n",
    "wqs_000d = cal_wqs(input_dir2, 0.90, 100)\n",
    "wqs_180d = cal_wqs(input_dir3, 0.90, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = #980 \n",
    "thres = #0.9999\n",
    "patience = #10\n",
    "\n",
    "def get_all_stable_cols(wqs, seed, thres, patience):\n",
    "    stable_cols = []\n",
    "    for wq in wqs:\n",
    "        i = stepwise.find_stable_corr(wq, seed, thres, patience, out_dir=None) \n",
    "        #find_stable #find_stable_corr\n",
    "        #i =  4 \n",
    "        \n",
    "        stable_col = wq[i][seed]\n",
    "\n",
    "        #stable_col = stepwise.min_max(stable_col) \n",
    "\n",
    "        stable_col = stepwise.z_score(stable_col) \n",
    "\n",
    "        stable_cols.append(stable_col)\n",
    "    return np.array(stable_cols)\n",
    "\n",
    "stable_ncnc = get_all_stable_cols(wqs_ncnc, seed, thres, patience)\n",
    "stable_000d = get_all_stable_cols(wqs_000d, seed, thres, patience)\n",
    "stable_180d = get_all_stable_cols(wqs_180d, seed, thres, patience)\n",
    "\n",
    "\n",
    "####################################################################################################################\n",
    "from scipy.stats import ttest_rel\n",
    "from datasets import mask\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "m = mask.NiiMask(r\"E:\\006_ET_MRgFUS\\09_gradient_batch2\\02_nii_mask\\1039atlas.nii\")\n",
    "\n",
    "ts, ps = ttest_rel(stable_180d, stable_000d)\n",
    "ts_filtered = np.multiply(ts, ps<0.05)\n",
    "tp = dict(zip(range(1, np.shape(ts)[0]+1), ts_filtered))\n",
    "\n",
    "print(f'ET180d_vs_ET000d Significant ROI count:{np.sum(ps<0.05)}') \n",
    "\n",
    "m.save_values(tp, out_path=r'E:\\006_ET_MRgFUS\\12_gradient_batch2.1\\15_SFC的稳定态度的数值\\ET180d_vs_ET000d_tvalue_SFC.nii') \n",
    "\n",
    "df = pd.DataFrame({'roi': range(1, np.shape(ts)[0]+1), 't_value': ts, 'p_value': ps})\n",
    "df.to_csv(r'E:\\006_ET_MRgFUS\\12_gradient_batch2.1\\15_SFC的稳定态度的数值\\Part4.1_ET180d_vs_ET000d_SFC.csv') \n",
    "\n",
    "###################################################################################################################\n",
    "\n",
    "ts, ps = ttest_rel(stable_000d, stable_ncnc)\n",
    "ts_filtered = np.multiply(ts, ps<0.05)\n",
    "tp = dict(zip(range(1, np.shape(ts)[0]+1), ts_filtered))\n",
    "\n",
    "print(f'ET180d_vs_ET000d Significant ROI count:{np.sum(ps<0.05)}') \n",
    "\n",
    "m.save_values(tp, out_path=r'E:\\006_ET_MRgFUS\\12_gradient_batch2.1\\15_SFC的稳定态度的数值\\ET000d_vs_ncncnc_tvalue_SFC.nii') \n",
    "\n",
    "df = pd.DataFrame({'roi': range(1, np.shape(ts)[0]+1), 't_value': ts, 'p_value': ps})\n",
    "df.to_csv(r'E:\\006_ET_MRgFUS\\12_gradient_batch2.1\\15_SFC的稳定态度的数值\\Part4.1_ET000d_vs_ncncnc_SFC.csv') \n",
    "\n",
    "###################################################################################################################\n",
    "\n",
    "ts, ps = ttest_rel(stable_180d, stable_ncnc)\n",
    "ts_filtered = np.multiply(ts, ps<0.05)\n",
    "tp = dict(zip(range(1, np.shape(ts)[0]+1), ts_filtered))\n",
    "\n",
    "print(f'ET180d_vs_ET000d Significant ROI count:{np.sum(ps<0.05)}') \n",
    "\n",
    "m.save_values(tp, out_path=r'E:\\006_ET_MRgFUS\\12_gradient_batch2.1\\15_SFC的稳定态度的数值\\ET180d_vs_ncncnc_tvalue_SFC.nii') \n",
    "\n",
    "df = pd.DataFrame({'roi': range(1, np.shape(ts)[0]+1), 't_value': ts, 'p_value': ps})\n",
    "df.to_csv(r'E:\\006_ET_MRgFUS\\12_gradient_batch2.1\\15_SFC的稳定态度的数值\\Part4.1_ET180d_vs_ncncnc_SFC.csv') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "from datasets import mask\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "seed = 980 \n",
    "thres = 0.9999\n",
    "patience = 10\n",
    "\n",
    "def get_all_stable_cols(axi, wqs, seed, thres, patience):\n",
    "    stable_cols = []\n",
    "    for wq in wqs:\n",
    "        #i = stepwise.find_stable_corr(wq, seed, thres, patience, out_dir=None) \n",
    "        #i =  8\n",
    "        #find_stable #find_stable_corr\n",
    "        i=axi\n",
    "\n",
    "        stable_col = wq[i][seed]\n",
    "\n",
    "        stable_cols.append(stable_col)\n",
    "    return np.array(stable_cols)\n",
    "\n",
    "for i in range(40, 42):\n",
    "    axi = i\n",
    "\n",
    "    print(\"#####################################################\")\n",
    "    print(\"STEP\", axi)\n",
    "    stable_ncnc = get_all_stable_cols(axi, wqs_ncnc, seed, thres, patience)\n",
    "    stable_000d = get_all_stable_cols(axi, wqs_000d, seed, thres, patience)\n",
    "    stable_180d = get_all_stable_cols(axi, wqs_180d, seed, thres, patience)\n",
    "\n",
    "    m = mask.NiiMask(r\"E:\\006_ET_MRgFUS\\09_gradient_batch2\\02_nii_mask\\1039atlas.nii\")\n",
    "\n",
    "    ts, ps = ttest_rel(stable_180d, stable_000d)\n",
    "    ts_filtered = np.multiply(ts, ps<0.0004)\n",
    "    tp = dict(zip(range(1, np.shape(ts)[0]+1), ts_filtered))\n",
    "    ts_filtered = np.nan_to_num(ts_filtered)\n",
    "    print(np.argmax(ts_filtered))\n",
    "    print(f'ET180d_vs_ET000d Significant ROI count:{np.sum(ps<0.0004)}') \n",
    "\n",
    "    ###################################################################################################################\n",
    "\n",
    "    #ET000d_vs_ncnc脑区的梯度效应值进行统计，并保存\n",
    "    ts, ps = ttest_rel(stable_000d, stable_ncnc)\n",
    "    ts_filtered = np.multiply(ts,  ps<0.0004)\n",
    "    tp = dict(zip(range(1, np.shape(ts)[0]+1), ts_filtered))\n",
    "    ts_filtered = np.nan_to_num(ts_filtered)\n",
    "    print(np.argmax(ts_filtered))\n",
    "    print(f'ET000d_vs_ncnc Significant ROI count:{np.sum(ps<0.0004)}') \n",
    "\n",
    "    ###################################################################################################################\n",
    "    ts, ps = ttest_rel(stable_180d, stable_ncnc)\n",
    "    ts_filtered = np.multiply(ts,  ps<0.0004)\n",
    "    tp = dict(zip(range(1, np.shape(ts)[0]+1), ts_filtered))\n",
    "    ts_filtered = np.nan_to_num(ts_filtered)\n",
    "    print(np.argmax(ts_filtered))\n",
    "\n",
    "    print(f'ET180d_vs_ncnc Significant ROI count:{np.sum(ps<0.0004)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "seed = 980 \n",
    "thres = 0.99\n",
    "patience = 15\n",
    "\n",
    "\n",
    "def get_all_stable_cols(axi, wqs, seed, thres, patience):\n",
    "    stable_cols = []\n",
    "    for wq in wqs:\n",
    "\n",
    "        i=axi\n",
    "\n",
    "        stable_col = wq[i][seed]\n",
    "        \n",
    "        stable_cols.append(stable_col)\n",
    "    return np.array(stable_cols)\n",
    "\n",
    "from scipy.stats import ttest_rel,ttest_ind\n",
    "\n",
    "for axi in range(0, 100):\n",
    "    stable_ncnc = get_all_stable_cols(axi, wqs_ncnc, seed, thres, patience)\n",
    "    stable_000d = get_all_stable_cols(axi, wqs_000d, seed, thres, patience)\n",
    "    stable_180d = get_all_stable_cols(axi, wqs_180d, seed, thres, patience)\n",
    "\n",
    "    ts, ps = ttest_rel(stable_ncnc, stable_000d)\n",
    "    df = pd.DataFrame({'roi': range(1, np.shape(ts)[0]+1), 't_value': ts, 'p_value': ps})\n",
    "    \n",
    "    print(\"###########################\")\n",
    "    print(\"step\", axi+1)  \n",
    "    print(df)\n",
    "    df.to_csv(f'E:/006_ET_MRgFUS/12_gradient_batch2.1/step{axi+1}.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mytoolkit2 import stepwise\n",
    "import numpy as np\n",
    "\n",
    "def cal_wqs(input_dir, thres, step):\n",
    "    files = os.listdir(input_dir)\n",
    "    wqs = []\n",
    "\n",
    "    for f in files:\n",
    "        array= np.loadtxt(os.path.join(input_dir, f))\n",
    "        #array[array<0] = 0\n",
    "       \n",
    "        #array = stepwise.sparsity(array, thres=thres)\n",
    "        array = stepwise.row_sparsity(array, thres=thres)\n",
    "      \n",
    "        wq, *_ = stepwise.findwalks_v2(array, step, normalize=stepwise.min_max)\n",
    "        wqs.append(wq)\n",
    "    return wqs\n",
    "\n",
    "input_dir1 = r'E:\\006_ET_MRgFUS\\09_gradient_batch2\\01_zform_data_group\\ncnc'\n",
    "input_dir2 = r'E:\\006_ET_MRgFUS\\09_gradient_batch2\\01_zform_data_group\\000d'\n",
    "input_dir3 = r'E:\\006_ET_MRgFUS\\09_gradient_batch2\\01_zform_data_group\\180d'\n",
    "\n",
    "wqs_ncnc = cal_wqs(input_dir1, 0.90, 100)\n",
    "wqs_000d = cal_wqs(input_dir2, 0.90, 100)\n",
    "wqs_180d = cal_wqs(input_dir3, 0.90, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "seed = 980 #对应脑区减1\n",
    "thres = 0.99\n",
    "patience = 15 #10~20\n",
    "\n",
    "def get_all_stable_cols(wqs, seed, thres, patience):\n",
    "    stable_cols = []\n",
    "    for wq in wqs:\n",
    "        i = stepwise.find_stable_corr(wq, seed, thres, patience, out_dir=None)\n",
    "        #i =  8 \n",
    "        #find_stable #find_stable_corr\n",
    "        #print(i)\n",
    "        stable_col = wq[i][seed]\n",
    "\n",
    "        #stable_col = stepwise.min_max(stable_col) \n",
    "        #stable_col = stepwise.z_score(stable_col) \n",
    "        \n",
    "        stable_cols.append(stable_col)\n",
    "    return np.array(stable_cols)\n",
    "\n",
    "#print('NNCNC')\n",
    "stable_ncnc = get_all_stable_cols(wqs_ncnc, seed, thres, patience)\n",
    "#print('000D')\n",
    "stable_000d = get_all_stable_cols(wqs_000d, seed, thres, patience)\n",
    "#print('180D')\n",
    "stable_180d = get_all_stable_cols(wqs_180d, seed, thres, patience)\n",
    "\n",
    "#############################################################################################################\n",
    "\n",
    "from scipy.stats import ttest_rel,ttest_ind\n",
    "from mytoolkit1.ttest import ind_cohen_d, rel_cohen_d\n",
    "\n",
    "subnetwork_path = r'E:\\006_ET_MRgFUS\\09_gradient_batch2\\02_nii_mask\\7network_atlas1039.csv'\n",
    "subnetwork_df = pd.read_csv(subnetwork_path)\n",
    "subnetwork_labels = subnetwork_df['Subnetwork'].values\n",
    "\n",
    "def get_subnetwork_mean(sub_col, subnetwork_labels):\n",
    "    means = []\n",
    "    for j in range(1, 8):\n",
    "        subnetwork_value = sub_col[subnetwork_labels==j]\n",
    "        mean = np.nanmean(subnetwork_value)\n",
    "        means.append(mean)\n",
    "    return means\n",
    "\n",
    "def get_subnetwork_means(subs_col, subnetwork_labels):\n",
    "    subs_means = []\n",
    "    for sub_col in subs_col:\n",
    "        means = get_subnetwork_mean(sub_col, subnetwork_labels)\n",
    "        subs_means.append(means)\n",
    "    return np.array(subs_means)\n",
    "\n",
    "subs_means_ncnc = get_subnetwork_means(stable_ncnc, subnetwork_labels)\n",
    "subs_means_000d = get_subnetwork_means(stable_000d, subnetwork_labels)\n",
    "subs_means_180d = get_subnetwork_means(stable_180d, subnetwork_labels)\n",
    "\n",
    "ts, ps = ttest_rel(subs_means_180d, subs_means_000d)\n",
    "d = rel_cohen_d(subs_means_180d, subs_means_000d)\n",
    "df1 = pd.DataFrame({'roi': range(1, np.shape(ts)[0]+1), 't_value': ts, 'p_value': ps, 'd_value': d})\n",
    "print(df1)\n",
    "#df.to_csv(r'E:\\006_ET_MRgFUS\\14_SFC_analysis\\1.csv') \n",
    "\n",
    "ts, ps = ttest_ind(subs_means_000d, subs_means_ncnc)\n",
    "d = ind_cohen_d(subs_means_000d, subs_means_ncnc)\n",
    "df2 = pd.DataFrame({'roi': range(1, np.shape(ts)[0]+1), 't_value': ts, 'p_value': ps, 'd_value': d})\n",
    "print(df2)\n",
    "#df.to_csv(r'E:\\006_ET_MRgFUS\\14_SFC_analysis\\2.csv') \n",
    "\n",
    "ts, ps = ttest_ind(subs_means_180d, subs_means_ncnc)\n",
    "d = ind_cohen_d(subs_means_180d, subs_means_ncnc)\n",
    "df3 = pd.DataFrame({'roi': range(1, np.shape(ts)[0]+1), 't_value': ts, 'p_value': ps, 'd_value': d})\n",
    "print(df3)\n",
    "#df.to_csv(r'E:\\006_ET_MRgFUS\\14_SFC_analysis\\3.csv') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "seed = 980 \n",
    "thres = 0.9999 #0.99\n",
    "patience = 10  #15\n",
    "\n",
    "\n",
    "def get_all_stable_cols(axi, wqs, seed, thres, patience):\n",
    "    stable_cols = []\n",
    "    for wq in wqs:\n",
    "        #i = stepwise.find_stable_corr(wq, seed, thres, patience, out_dir=None)\n",
    "        #i =  8 \n",
    "        #find_stable #find_stable_corr\n",
    "        i=axi\n",
    "\n",
    "        stable_col = wq[i][seed]\n",
    "\n",
    "        ##stable_col = stepwise.min_max(stable_col)\n",
    "        #stable_col = stepwise.min_max(stable_col) \n",
    "        #stable_col = stepwise.z_score(stable_col) \n",
    "        \n",
    "        stable_cols.append(stable_col)\n",
    "    return np.array(stable_cols)\n",
    "\n",
    "from scipy.stats import ttest_rel,ttest_ind\n",
    "\n",
    "subnetwork_path = r'E:\\006_ET_MRgFUS\\09_gradient_batch2\\02_nii_mask\\7network_atlas1039.csv'\n",
    "subnetwork_df = pd.read_csv(subnetwork_path)\n",
    "subnetwork_labels = subnetwork_df['Subnetwork'].values\n",
    "\n",
    "\n",
    "def get_subnetwork_mean(sub_col, subnetwork_labels):\n",
    "    means = []\n",
    "    for j in range(1, 8):\n",
    "        subnetwork_value = sub_col[subnetwork_labels==j]\n",
    "        mean = np.nanmean(subnetwork_value)\n",
    "        means.append(mean)\n",
    "    return means\n",
    "\n",
    "def get_subnetwork_means(subs_col, subnetwork_labels):\n",
    "    subs_means = []\n",
    "    for sub_col in subs_col:\n",
    "        means = get_subnetwork_mean(sub_col, subnetwork_labels)\n",
    "        subs_means.append(means)\n",
    "    return np.array(subs_means)\n",
    "\n",
    "for axi in range(0, 100):\n",
    "    stable_ncnc = get_all_stable_cols(axi, wqs_ncnc, seed, thres, patience)\n",
    "    stable_000d = get_all_stable_cols(axi, wqs_000d, seed, thres, patience)\n",
    "    stable_180d = get_all_stable_cols(axi, wqs_180d, seed, thres, patience)\n",
    "\n",
    "    subs_means_ncnc = get_subnetwork_means(stable_ncnc, subnetwork_labels)\n",
    "    subs_means_000d = get_subnetwork_means(stable_000d, subnetwork_labels)\n",
    "    subs_means_180d = get_subnetwork_means(stable_180d, subnetwork_labels)\n",
    "\n",
    "    ts, ps = ttest_rel(subs_means_180d, subs_means_000d)\n",
    "    df = pd.DataFrame({'roi': range(1, np.shape(ts)[0]+1), 't_value': ts, 'p_value': ps})\n",
    "    \n",
    "    print(\"###########################\")\n",
    "    print(\"step\", axi+1)  \n",
    "    print(df)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6eac6ea3f87aa8a18101560350ba9d76665d8c267d107134f193807daf6a975"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
